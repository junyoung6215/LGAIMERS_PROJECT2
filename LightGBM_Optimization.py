print(">> [LightGBM_Optimization] νμΌ μ‹¤ν–‰ μ‹μ‘")  # νμΌ μ‹¤ν–‰ λ΅κ·Έ

import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
import optuna
import joblib
import os

# Step 1: λ°μ΄ν„° λ΅λ“ λ° λ¶„ν•  (νƒ€κ²: "μ„μ‹  μ„±κ³µ μ—¬λ¶€")
print("Step 1: λ°μ΄ν„° λ΅λ“ λ° λ¶„ν•  μ‹μ‘")
# train.csv νμΌμ„ μ½μ–΄μ¤κ³ , νƒ€κ² 'μ„μ‹  μ„±κ³µ μ—¬λ¶€'μ™€ νΉμ§• λ°μ΄ν„°λ¥Ό λ¶„λ¦¬ν•©λ‹λ‹¤.
data = pd.read_csv("train.csv")
if "ID" in data.columns:
    data = data.drop(columns=["ID"])
print("  [λ°μ΄ν„° λ΅λ“ μ™„λ£] train.csv νμΌ μ½κΈ° μ„±κ³µ")
X = data.drop(columns=["μ„μ‹  μ„±κ³µ μ—¬λ¶€"])
y = data["μ„μ‹  μ„±κ³µ μ—¬λ¶€"]

# λ²”μ£Όν• μ»¬λΌ μ²λ¦¬: category νƒ€μ…μΌλ΅ λ³€ν™
string_columns = X.select_dtypes(include=['object']).columns.tolist()
print("π” [LightGBM] λ²”μ£Όν• λ³€μ:", string_columns)
for col in string_columns:
    X[col] = X[col].fillna("missing")  # κ²°μΈ΅μΉ μ²λ¦¬
    X[col] = X[col].astype("category")  # category νƒ€μ…μΌλ΅ λ³€ν™
print("  [λ²”μ£Όν• λ³€μ λ³€ν™ μ™„λ£] category νƒ€μ…μΌλ΅ λ³€ν™λ¨")

# Stratified λ°©μ‹μΌλ΅ λ°μ΄ν„°λ¥Ό 80:20 λΉ„μ¨λ΅ λ¶„ν• ν•©λ‹λ‹¤.
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
print(f"  [λ°μ΄ν„° λ¶„ν•  μ™„λ£] ν•™μµ λ°μ΄ν„° shape: {X_train.shape}, ν…μ¤νΈ λ°μ΄ν„° shape: {X_test.shape}")

# ν΄λμ¤ λ¶κ· ν• ν™•μΈ λ° scale_pos_weight κ³„μ‚°
neg_count = (y_train == 0).sum()
pos_count = (y_train == 1).sum()
default_scale_pos_weight = neg_count / pos_count if pos_count > 0 else 1
print(f"  [ν΄λμ¤ λΉ„μ¨] μμ„±:{neg_count}, μ–‘μ„±:{pos_count}, κΈ°λ³Έ scale_pos_weight: {default_scale_pos_weight:.2f}\n")

def objective(trial):
    print(f">> [LightGBM] Trial {trial.number} μ‹μ‘")
    # boosting_typeμ„ gbdt λλ” dartλ΅ μ„ νƒν•μ—¬, dart μ„ νƒ μ‹ drop_rateλ„ νλ‹
    boost_type = trial.suggest_categorical("boosting_type", ["gbdt", "dart"])
    param = {
        "objective": "binary",
        "metric": "auc",
        "boosting_type": boost_type,
        "learning_rate": trial.suggest_float("learning_rate", 0.005, 0.3, log=True),
        "num_leaves": trial.suggest_int("num_leaves", 30, 200),  # λ²”μ„ μ„Έλ¶„ν™”
        "max_depth": trial.suggest_int("max_depth", 3, 15),        # λ°μ΄ν„°μ…‹μ— λ§κ² λ²”μ„ μ΅°μ •
        "min_data_in_leaf": trial.suggest_int("min_data_in_leaf", 10, 200),
        "min_data_in_bin": trial.suggest_int("min_data_in_bin", 3, 100),
        "max_bin": trial.suggest_int("max_bin", 100, 300),
        "lambda_l1": trial.suggest_float("lambda_l1", 1e-8, 20.0, log=True),
        "lambda_l2": trial.suggest_float("lambda_l2", 1e-8, 20.0, log=True),
        "feature_fraction": trial.suggest_float("feature_fraction", 0.5, 1.0),
        "bagging_fraction": trial.suggest_float("bagging_fraction", 0.5, 1.0),
        "bagging_freq": trial.suggest_int("bagging_freq", 1, 7),
        "colsample_bytree": trial.suggest_float("colsample_bytree", 0.5, 1.0)
    }
    
    # dart μ „μ© drop_rate μ¶”κ°€
    if boost_type == "dart":
        param["drop_rate"] = trial.suggest_float("drop_rate", 0.1, 0.5)
    
    # κΈ°μ΅΄λ³΄λ‹¤ μ„Έλ°€ν• num_boost_round λ²”μ„ μ μ©
    num_boost_round = trial.suggest_int("num_boost_round", 500, 2000, step=50)
    print(f"[Optuna] Trial {trial.number} μ„¤μ •λ νλΌλ―Έν„°: {param}, num_boost_round = {num_boost_round}")

    print(f">> [LightGBM] Trial {trial.number} - λ¨λΈ ν•™μµ μ‹μ‘")
    lgb_train = lgb.Dataset(X_train, label=y_train, categorical_feature=string_columns)
    lgb_eval = lgb.Dataset(X_test, label=y_test, categorical_feature=string_columns, reference=lgb_train)
    
    # Callback ν•¨μλ΅ early stopping μ‚¬μ©, verbose_eval μ κ±°
    callbacks = [
        lgb.early_stopping(stopping_rounds=50, verbose=False),
        lgb.log_evaluation(period=0)  # λ΅κ·Έ μ¶λ ¥ μ–µμ 
    ]
    
    gbm = lgb.train(
        param,
        lgb_train,
        num_boost_round=num_boost_round,
        valid_sets=[lgb_eval],
        callbacks=callbacks
    )
    
    print(f">> [LightGBM] Trial {trial.number} - μμΈ΅ λ° ν‰κ°€ μ§„ν–‰")
    y_prob = gbm.predict(X_test, num_iteration=gbm.best_iteration)
    auc = roc_auc_score(y_test, y_prob)
    print(f"[Optuna] Trial {trial.number} μ™„λ£: ROC-AUC = {auc}\n")
    return auc

# LightGBMμ© νλΌλ―Έν„° μ—…λ°μ΄νΈ κ³Όμ • μƒμ„Έ λ΅κ·Έ μ¶λ ¥
def optimize_lightgbm():
    best_param_file = "open/best_lightgbm_params.pkl"
    old_score = 0
    if os.path.exists(best_param_file):
        best_old = joblib.load(best_param_file)
        old_score = best_old.get("roc_auc", 0)
        print(f"[LightGBM LOG] κΈ°μ΅΄ νλΌλ―Έν„° λ΅λ“ μ„±κ³µ: {best_old}")
    else:
        print("[LightGBM LOG] κΈ°μ΅΄ νλΌλ―Έν„° νμΌμ΄ μ—†μ. μƒ νμΌ μƒμ„± μμ •.")
    
    study = optuna.create_study(direction="maximize")
    study.optimize(objective, n_trials=50)
    new_score = study.best_value
    print(f"[LightGBM LOG] μµμ ν™” μ™„λ£: μƒ ROC-AUC = {new_score:.4f}")
    
    if new_score > old_score:
        best_params = study.best_params
        best_params["roc_auc"] = new_score
        joblib.dump(best_params, best_param_file)
        print(f"[LightGBM LOG] κΈ°μ΅΄ νμΌλ³΄λ‹¤ λ‚μ€ μ„±λ¥ ({old_score:.4f} -> {new_score:.4f})μΌλ΅ μµμ  νλΌλ―Έν„°λ΅ μμ •ν•μ€μµλ‹λ‹¤: {best_params}")
    else:
        print(f"[LightGBM LOG] κΈ°μ΅΄ νλΌλ―Έν„° μ μ§€: κΈ°μ΅΄ ROC-AUC = {old_score:.4f} >= μƒ ROC-AUC = {new_score:.4f}")
    
    return study  # study κ°μ²΄λ¥Ό λ°ν™

print("Step 2: Optunaλ¥Ό ν†µν• ν•μ΄νΌνλΌλ―Έν„° μµμ ν™” μ‹μ‘")
study = optimize_lightgbm()
print(">> [LightGBM] μµμ ν™” μ™„λ£")

from sklearn.model_selection import StratifiedKFold
import numpy as np
from sklearn.metrics import roc_auc_score

def evaluate_ensemble_roc_kfold(X, y, model_params, n_splits=5):
    print(">> [LightGBM] K-Fold μ•™μƒλΈ” ν‰κ°€ ν•¨μ μ‹¤ν–‰")
    from sklearn.model_selection import StratifiedKFold
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
    oof_preds = np.zeros(len(y))
    auc_scores = []
    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):
        print(f"  [Fold {fold+1}/{n_splits}] ν›λ ¨ μ‹μ‘")
        X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]
        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]
        
        train_data = lgb.Dataset(X_train_fold, label=y_train_fold)
        val_data = lgb.Dataset(X_val_fold, label=y_val_fold, reference=train_data)
        
        # λ¨λΈ ν•™μµ (verbose_eval μ κ±°)
        model = lgb.train(model_params,
                          train_data,
                          valid_sets=[val_data])
        print(f"  [Fold {fold+1}] λ¨λΈ ν•™μµ μ™„λ£")
        
        # μμΈ΅ λ° ROC-AUC κ³„μ‚°
        y_pred = model.predict(X_val_fold)
        oof_preds[val_idx] = y_pred
        fold_auc = roc_auc_score(y_val_fold, y_pred)
        auc_scores.append(fold_auc)
        print(f"  [Fold {fold+1}] ROC-AUC: {fold_auc:.4f}")
        
    final_auc = roc_auc_score(y, oof_preds)
    print(f">> [LightGBM] μµμΆ… ROC-AUC: {final_auc:.4f}")
    print(f">> [LightGBM] ν‰κ·  Fold ROC-AUC: {np.mean(auc_scores):.4f}, ν‘μ¤€νΈμ°¨: {np.std(auc_scores):.4f}")
    return final_auc, oof_preds, auc_scores

# μ•™μƒλΈ” ν‰κ°€ μ½”λ“ μ¶”κ°€
print("\n>> [LightGBM] K-Fold κΈ°λ° μ•™μƒλΈ” μ„±λ¥ ν‰κ°€")
ensemble_auc, oof_predictions, fold_scores = evaluate_ensemble_roc_kfold(X, y, study.best_params, n_splits=5)

print("\n=== μµμΆ… μ„±λ¥ λΉ„κµ ===")
print(f"λ‹¨μΌ λ¨λΈ ROC-AUC: {study.best_value:.4f}")
print(f"K-Fold μ•™μƒλΈ” ROC-AUC: {ensemble_auc:.4f}")
print(f"μ„±λ¥ μ°¨μ΄: {(ensemble_auc - study.best_value):.4f}")

# 'open' λ””λ ‰ν† λ¦¬ μ΅΄μ¬ ν™•μΈ λ° μƒμ„±
if not os.path.exists('open'):
    os.makedirs('open')
    print(">> [LightGBM] 'open' λ””λ ‰ν† λ¦¬ μƒμ„±λ¨")

# μµμ  νλΌλ―Έν„° μ €μ¥ (LightGBM μ „μ© νμΌλ…μΌλ΅ μμ •)
best_params_path = "open/best_lightgbm_params.pkl"
joblib.dump(study.best_params, best_params_path)
print(f">> [LightGBM] μµμ  νλΌλ―Έν„° μ €μ¥ μ™„λ£: {os.path.abspath(best_params_path)}")

# OOF μμΈ΅κ°’ μ €μ¥
oof_df = pd.DataFrame({'true_values': y, 'oof_predictions': oof_predictions})
oof_df.to_csv('open/lightgbm_oof_predictions.csv', index=False)
print(">> [LightGBM] OOF μμΈ΅κ°’ μ €μ¥ μ™„λ£")
print(">> [LightGBM_Optimization] νμΌ μ‹¤ν–‰ μΆ…λ£")
