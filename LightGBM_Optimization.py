import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
import optuna
import joblib

# Step 1: ë°ì´í„° ë¡œë“œ ë° ë¶„í•  (íƒ€ê²Ÿ: "ì„ì‹  ì„±ê³µ ì—¬ë¶€")
print("Step 1: ë°ì´í„° ë¡œë“œ ë° ë¶„í•  ì‹œì‘")
# train.csv íŒŒì¼ì„ ì½ì–´ì˜¤ê³ , íƒ€ê²Ÿ 'ì„ì‹  ì„±ê³µ ì—¬ë¶€'ì™€ íŠ¹ì§• ë°ì´í„°ë¥¼ ë¶„ë¦¬í•©ë‹ˆë‹¤.
data = pd.read_csv("train.csv")
print("  [ë°ì´í„° ë¡œë“œ ì™„ë£Œ] train.csv íŒŒì¼ ì½ê¸° ì„±ê³µ")
X = data.drop(columns=["ì„ì‹  ì„±ê³µ ì—¬ë¶€"])
y = data["ì„ì‹  ì„±ê³µ ì—¬ë¶€"]

# ë²”ì£¼í˜• ì»¬ëŸ¼ ì²˜ë¦¬: category íƒ€ì…ìœ¼ë¡œ ë³€í™˜
string_columns = X.select_dtypes(include=['object']).columns.tolist()
print("ğŸ” [LightGBM] ë²”ì£¼í˜• ë³€ìˆ˜:", string_columns)
for col in string_columns:
    X[col] = X[col].fillna("missing")  # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    X[col] = X[col].astype("category")  # category íƒ€ì…ìœ¼ë¡œ ë³€í™˜
print("  [ë²”ì£¼í˜• ë³€ìˆ˜ ë³€í™˜ ì™„ë£Œ] category íƒ€ì…ìœ¼ë¡œ ë³€í™˜ë¨")

# Stratified ë°©ì‹ìœ¼ë¡œ ë°ì´í„°ë¥¼ 80:20 ë¹„ìœ¨ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
print(f"  [ë°ì´í„° ë¶„í•  ì™„ë£Œ] í•™ìŠµ ë°ì´í„° shape: {X_train.shape}, í…ŒìŠ¤íŠ¸ ë°ì´í„° shape: {X_test.shape}")

# í´ë˜ìŠ¤ ë¶ˆê· í˜• í™•ì¸ ë° scale_pos_weight ê³„ì‚°
neg_count = (y_train == 0).sum()
pos_count = (y_train == 1).sum()
default_scale_pos_weight = neg_count / pos_count if pos_count > 0 else 1
print(f"  [í´ë˜ìŠ¤ ë¹„ìœ¨] ìŒì„±:{neg_count}, ì–‘ì„±:{pos_count}, ê¸°ë³¸ scale_pos_weight: {default_scale_pos_weight:.2f}\n")

def objective(trial):
    print(f">> [LightGBM] Trial {trial.number} ì‹œì‘")
    # LightGBMì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ë²”ìœ„ ì„¤ì •
    param = {
        "objective": "binary",
        "metric": "auc",
        "boosting_type": "gbdt",
        "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
        "num_leaves": trial.suggest_int("num_leaves", 20, 150),
        "max_depth": trial.suggest_int("max_depth", 3, 15),
        "min_data_in_leaf": trial.suggest_int("min_data_in_leaf", 20, 100),
        "lambda_l1": trial.suggest_float("lambda_l1", 1e-8, 10.0, log=True),
        "lambda_l2": trial.suggest_float("lambda_l2", 1e-8, 10.0, log=True),
        "feature_fraction": trial.suggest_float("feature_fraction", 0.4, 1.0),
        "bagging_fraction": trial.suggest_float("bagging_fraction", 0.4, 1.0),
        "bagging_freq": trial.suggest_int("bagging_freq", 1, 7),
        "scale_pos_weight": trial.suggest_float("scale_pos_weight", 0.5, 2.0, step=0.1)
    }
    num_boost_round = trial.suggest_int("num_boost_round", 100, 1000, step=50)
    print(f"[Optuna] Trial {trial.number} ì„¤ì •ëœ íŒŒë¼ë¯¸í„°: {param}, num_boost_round = {num_boost_round}")

    print(f">> [LightGBM] Trial {trial.number} - ëª¨ë¸ í•™ìŠµ ì‹œì‘")
    lgb_train = lgb.Dataset(X_train, label=y_train, categorical_feature=string_columns)
    lgb_eval = lgb.Dataset(X_test, label=y_test, categorical_feature=string_columns, reference=lgb_train)
    
    # Use callback functions for early stopping and logging control
    callbacks = [
        lgb.early_stopping(stopping_rounds=50, verbose=False),
        lgb.log_evaluation(period=0)  # Suppress training log output
    ]
    
    gbm = lgb.train(
        param,
        lgb_train,
        num_boost_round=num_boost_round,
        valid_sets=[lgb_eval],
        callbacks=callbacks
    )
    
    print(f">> [LightGBM] Trial {trial.number} - ì˜ˆì¸¡ ë° í‰ê°€ ì§„í–‰")
    y_prob = gbm.predict(X_test, num_iteration=gbm.best_iteration)
    auc = roc_auc_score(y_test, y_prob)
    print(f"[Optuna] Trial {trial.number} ì™„ë£Œ: ROC-AUC = {auc}\n")
    return auc

print("Step 2: Optunaë¥¼ í†µí•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘")
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=5)
print(">> [LightGBM] ìµœì í™” ì™„ë£Œ")
print("ìµœì ì˜ ROC-AUC:", study.best_value)
print("ìµœì ì˜ íŒŒë¼ë¯¸í„°:", study.best_params)

# 'open' ë””ë ‰í† ë¦¬ ì¡´ì¬ í™•ì¸ ë° ìƒì„±
import os
if not os.path.exists('open'):
    os.makedirs('open')
    print(">> [LightGBM] 'open' ë””ë ‰í† ë¦¬ ìƒì„±ë¨")

# ìµœì  íŒŒë¼ë¯¸í„° ì €ì¥ (LightGBM ì „ìš© íŒŒì¼ëª…ìœ¼ë¡œ ìˆ˜ì •)
best_params_path = "open/best_lgbm_params.pkl"
joblib.dump(study.best_params, best_params_path)
print(f">> [LightGBM] ìµœì  íŒŒë¼ë¯¸í„° ì €ì¥ ì™„ë£Œ: {os.path.abspath(best_params_path)}")
