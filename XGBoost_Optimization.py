import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
from sklearn.preprocessing import LabelEncoder
import optuna
import joblib
import os

# Step 1: λ°μ΄ν„° λ΅λ“ λ° μ „μ²λ¦¬
print("Step 1: λ°μ΄ν„° λ΅λ“ λ° λ¶„ν•  μ‹μ‘")
data = pd.read_csv("train.csv")
X = data.drop(columns=["μ„μ‹  μ„±κ³µ μ—¬λ¶€"])
y = data["μ„μ‹  μ„±κ³µ μ—¬λ¶€"]

# λ²”μ£Όν• λ³€μ μ²λ¦¬
cat_features = X.select_dtypes(include=['object']).columns
print("π” [XGBoost] λ²”μ£Όν• λ³€μ:", cat_features.tolist())

# κ²°μΈ΅μΉλ¥Ό 'missing'μΌλ΅ μ±„μ°κ³  Label Encoding μ μ©
label_encoders = {}
for col in cat_features:
    X[col] = X[col].fillna("missing")
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col].astype(str))
    label_encoders[col] = le
print("  [λ²”μ£Όν• λ³€μ λ³€ν™ μ™„λ£] Label encoding μ μ©λ¨")
print(f"  [λ³€ν™ ν›„ νΉμ„± μ] {X.shape[1]}κ°")

# λ°μ΄ν„° λ¶„ν• 
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
print(f"  [λ°μ΄ν„° λ¶„ν•  μ™„λ£] ν•™μµ λ°μ΄ν„° shape: {X_train.shape}, ν…μ¤νΈ λ°μ΄ν„° shape: {X_test.shape}")

# ν΄λμ¤ λ¶κ· ν• ν™•μΈ
neg_count = (y_train == 0).sum()
pos_count = (y_train == 1).sum()
default_scale_pos_weight = neg_count / pos_count if pos_count > 0 else 1
print(f"  [ν΄λμ¤ λΉ„μ¨] μμ„±:{neg_count}, μ–‘μ„±:{pos_count}, κΈ°λ³Έ scale_pos_weight: {default_scale_pos_weight:.2f}\n")

def objective(trial):
    print(f">> [XGBoost] Trial {trial.number} μ‹μ‘")
    param = {
        "objective": "binary:logistic",
        "eval_metric": "auc",
        "booster": "gbtree",
        "max_depth": trial.suggest_int("max_depth", 4, 10),
        "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.2, log=True),
        "n_estimators": trial.suggest_int("n_estimators", 200, 800, step=50),
        "min_child_weight": trial.suggest_int("min_child_weight", 3, 10),
        "subsample": trial.suggest_float("subsample", 0.7, 0.9),
        "colsample_bytree": trial.suggest_float("colsample_bytree", 0.7, 0.9),
        "scale_pos_weight": trial.suggest_float("scale_pos_weight", 0.5, 2.0, step=0.05),
        "gamma": trial.suggest_float("gamma", 0.0, 5.0),
        "lambda": trial.suggest_float("lambda", 1e-8, 10.0, log=True),
        "alpha": trial.suggest_float("alpha", 1e-8, 10.0, log=True),
        "random_state": 42
    }
    print(f"[Optuna] Trial {trial.number} μ„¤μ •λ νλΌλ―Έν„°: {param}")
    
    # early_stopping_rounds μ κ±°ν•μ—¬ fit μν–‰
    model = xgb.XGBClassifier(**param)
    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)
    
    y_prob = model.predict_proba(X_test)[:, 1]
    auc = roc_auc_score(y_test, y_prob)
    print(f"[Optuna] Trial {trial.number} μ™„λ£: ROC-AUC = {auc}\n")
    return auc

print("Step 2: Optunaλ¥Ό ν†µν• ν•μ΄νΌνλΌλ―Έν„° μµμ ν™” μ‹μ‘")
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=100)
print(">> [XGBoost] μµμ ν™” μ™„λ£")
print("μµμ μ ROC-AUC:", study.best_value)
print("μµμ μ νλΌλ―Έν„°:", study.best_params)

# 'open' λ””λ ‰ν† λ¦¬ μ΅΄μ¬ ν™•μΈ λ° μƒμ„±
if not os.path.exists('open'):
    os.makedirs('open')
    print(">> [XGBoost] 'open' λ””λ ‰ν† λ¦¬ μƒμ„±λ¨")

# μµμ  νλΌλ―Έν„° μ €μ¥
best_params_path = "open/best_xgb_params.pkl"
joblib.dump(study.best_params, best_params_path)
print(f">> [XGBoost] μµμ  νλΌλ―Έν„° μ €μ¥ μ™„λ£: {os.path.abspath(best_params_path)}")
